{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ijg5wUCTQYG"
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/python-live-training-template/blob/master/assets/datacamp.svg?raw=True\" alt = \"DataCamp icon\" width=\"50%\">\n",
    "</p>\n",
    "<br><br>\n",
    "\n",
    "This live training will review the concepts of statistical inference laid out in the DataCamp courses [Statistical Thinking in Python I](https://learn.datacamp.com/courses/statistical-thinking-in-python-part-1) and [Statistical Thinking in Python II](https://learn.datacamp.com/courses/statistical-thinking-in-python-part-2). The concepts of those courses were reviewed and fortified in [Case Studies in Statistical Thinking](https://learn.datacamp.com/courses/case-studies-in-statistical-thinking). You can link to those courses by clicking on the badges below.\n",
    "\n",
    "<div style=\"margin: auto; width: 400px\">\n",
    "\n",
    "<p>\n",
    "<a href=\"https://learn.datacamp.com/courses/statistical-thinking-in-python-part-1\"><img src = \"https://assets.datacamp.com/production/course_1549/shields/original/shield_image_course_1549_20191010-1-13inj9n?1570728356\" width=\"100px\"></a>\n",
    "<a href=\"https://learn.datacamp.com/courses/statistical-thinking-in-python-part-2\"><img src = \"https://assets.datacamp.com/production/course_1550/shields/original/shield_image_course_1550_20180911-13-4iztc?1536680442\" width=\"100px\"></a>\n",
    "<a href=\"https://learn.datacamp.com/courses/case-studies-in-statistical-thinking\"><img src = \"https://assets.datacamp.com/production/course_4674/shields/original/shield_image_course_4674_20190318-13-1c5k8sf?1552948884\" width=\"100px\"></a> \n",
    "</p>\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "Our goal here is to reinforce the concepts and techniques of statistical inference using hacker stats to help students gain confidence applying them to new data analysis tasks. In this part of the training, we will:\n",
    "\n",
    "- Review fundamental notions of probability and clarify the tasks of statistical inference.\n",
    "- Perform graphical exploratory data analysis (EDA).\n",
    "- Review concepts of confidence intervals.\n",
    "- Apply bootstrap methods for computing confidence intervals.\n",
    "\n",
    "<!-- In the second live training on hacker stats, we will:\n",
    "\n",
    "- Review concepts behind null hypothesis significance tests (NHSTs).\n",
    "- Construct confidence intervals and perform NHSTs comparing two treatments in a data set.\n",
    "- Compute confidence intervals on two-dimensional data sets, investigating correlations and linear regressions.\n",
    " -->\n",
    " \n",
    "We will do all of this using a data set (described [below](#The-Dataset)) which explores how sleep deprivation affects facial recognition tasks in humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Necessary packages**\n",
    "\n",
    "It is good practice to always have necessary package imports at the top of any `.py` file or notebook, so let's get our imports in before moving on.\n",
    "\n",
    "Unless I'll be plotting very large data sets (which is not the case in this webinar), I generally also like to set the figure format to be SVG. This avoids a pixelated look to the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMQfyC7GUNhT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# I want crisp graphics, so we'll output SVG\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Statistical inference**\n",
    "\n",
    "In this section of the notebook, I will discuss the goals of statistical inference and lay out the approach we will take. You may have seen these ideas presented before, but in my experience, investigating a topic from multiple angles leads to a much fuller understanding, and that is my goal here.\n",
    "\n",
    "We will dive into our (rather fun) data set momentarily, but for now, we will go over some important theoretical background to make sure we understand what statistical inference is all about, and how the hacker stats approach works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is probability?**\n",
    "\n",
    "To think about what probability means, consider an example. Say I am interested in the heights of American adult women. If we were to select an American woman at random, I think we would all agree that she is probably going to be under 190 cm (about 6' 3\"). We just used the word \"probably!\" To see what we mean by that, let's pick another woman at random. And another. And another. And another. We do this over and over and over. Eventually, we will pick a woman who is over 190 cm tall (for example basketball legend [Lisa Leslie](https://en.wikipedia.org/wiki/Lisa_Leslie) is 196 cm). So, the probability that a randomly chosen woman is over 190 cm is not zero, but is is small.\n",
    "\n",
    "We can directly apply this mode of thinking to give us an interpretation of probability, referred to as the **frequentist interpretation of probability**. The probability of an observation represents a long-run frequency over a large number of identical repetitions of a measurement/data collection. These repetitions can be, and often are, hypothetical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Probability distributions and CDFs**\n",
    "\n",
    "A **probability distribution** provides the link between observations to probability. For example a Normal distribution might link women's heights to probability. It is highly probable that a woman is between 150 and 180 cm (about five and six feet) tall, but improbable that a woman is above 190 cm.\n",
    "\n",
    "These statements of probability stem from the **cumulative distribution function** (CDF) associated with a probability distribution. The CDF evaluated at _x_ is defined as follows.\n",
    "\n",
    ">CDF(_x_) = probability of observing a value less than or equal to _x_.\n",
    "\n",
    "To get an idea of what a CDF might look like when we plot it, we can plot the CDF for a Normal distribution, say describing the height of American adult women. They average 165 cm, with a standard deviation of 9 cm. The mathematical expression for the CDF is complicated but known, and we can use the built-in functions in `scipy.stats` to generate the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-values for height CDF\n",
    "\n",
    "# Values of the CDF\n",
    "\n",
    "# Make the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the CDF evaluated at a given position along the x-axis is the probability of having an observation less than or equal to _x_. So, if we look at _x_ = 160 cm, we see that the probability that a woman is shorter than 160 cm is about 0.3.\n",
    "\n",
    "Importantly, the CDF contains all of the information about a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generative distributions**\n",
    "\n",
    "Say we did the experiment of measuring women's heights. For cost reasons (time and money), we can measure 100 women to get 100 height measurements. We might expect about 30 of these measurements to be below 160 cm and, based on the CDF above, about 7 of these measurements to be above 180 cm. This assumes that the distribution that provides that link between height measurements and probability is in fact a Normal distribution. If this is true, then the Normal distribution *generates* the data; it is the **generative distribution**.\n",
    "\n",
    "If we *know* the true generative distribution, then drawing random numbers out of the distribution is the same as performing the measurements themselves. We can draw random numbers out of a probability distribution using the `numpy.random` module. So, we can \"repeat\" the measurement of 100 women by drawing out of the generative distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility, seed the random number generator\n",
    "np.random.seed(3252)\n",
    "\n",
    "# Draw 100 measurments from a normal with mean 165 and std 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1> Q&A 1</h1> </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The empirical cumulative distribution function**\n",
    "\n",
    "The **empirical cumulative distribution function**, or ECDF, is a useful plot to make when doing exploratory data analysis. The ECDF at position _x_ is defined as\n",
    "\n",
    ">ECDF(_x_) = fraction of data points â‰¤ _x_.\n",
    "\n",
    "Compare this to the definition of the CDF.\n",
    "\n",
    ">CDF(_x_) = probability of observing a value less than or equal to _x_.\n",
    "\n",
    "Think about the frequentist interpretation of probability. If we had many, many measurements, then the fraction of data points less than or equal to _x_ is in fact the *probability* of observing a value less than or equal to _x_. So, the ECDF is close to the CDF. The differences between an ECDF and the CDF of the generative distribution are due entirely to the fact that we have only a finite number of measurements for the ECDF.\n",
    "\n",
    "To plot an ECDF, we position a dot for each data point where the x-value is the value of the data point itself and the y-value is the fraction of data points less or equal to the x-value. We can write a function to generate the x- and y-values for the ECDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    # Number of data points\n",
    "    \n",
    "    # x-data for the ECDF\n",
    "\n",
    "    # y-data for the ECDF\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to add the ECDF to our plot of the generative CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECDF\n",
    "\n",
    "# Make the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the ECDF closely matches the CDF, but we do see variation from it, owing to the small sample size of 100. If we increase the sample size to 1000, the ECDF follows more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw more heights\n",
    "\n",
    "# Compute ECDF\n",
    "\n",
    "# Make the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The task of statistical inference**\n",
    "\n",
    "In the above example, we knew the generative distribution. If we did actually know the generative distribution ahead of time, there would be no point in doing measurements. We generally do not know what the generative distribution is, so we collect data. **The task of statistical inference is to deduce the properties of a generative distribution of data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The plug-in principle**\n",
    "\n",
    "We could imagine that we knew (or strongly suspected) that the generative distribution for American women's heights is Normal, but we do not know the parameters. In this case, the statistical inference task is largely to determine what the parameters of the generative distribution are.\n",
    "\n",
    "But imagine we do not know anything a priori about the generative distribution. We still make measurements, and the measurements are all we have. We do not know anything about the CDF, but we do know about the ECDF. As the CDF defines a generative distribution, so too does the ECDF define an **empirical distribution**. We can proceed with statistical inference by using the empirical distribution in place of the (unknown) generative distribution. This approximation is referred to as the **plug-in** principle. This principle underlies many **non-parametric** approaches to statistical inference, so named because we are not trying to find parameters of a generative distribution, but are using only information from the data themselves. Application of the plug-in principle is the approach we take in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Properties of distributions via plug-in estimates**\n",
    "\n",
    "One-dimensional distributions have properties you may have heard of, like means, medians, and variances. Imagine we have a set of measurements stored in a NumPy array `x`. Then, the plug-in estimates for the various properties of the generative distribution are shown in the table below.\n",
    "\n",
    "| Property           | Plug-in estimate      |\n",
    "|--------------------|-----------------------|\n",
    "| mean               | `np.mean(x)`          |\n",
    "| median             | `np.median(x)`        |\n",
    "| variance           | `np.var(x)`           |\n",
    "| standard deviation | `np.std(x)`           |\n",
    "| _p_ percentile     | `np.percentile(x, p)` |\n",
    "\n",
    "\n",
    "\n",
    "Two-dimensional distribution additionally have properties like covariances and correlations. We can also estimate these using the plug-in principle. Let `x` and `y` be NumPy arrays. Then, for two-dimensional distributions, the plug-in estimates are:\n",
    "\n",
    "| Property           | Plug-in estimate              |\n",
    "|--------------------|-------------------------------|\n",
    "| covariance         | `np.cov(x, y, ddof=0)[0, 1]`  |\n",
    "| correlation        | `np.corrcoef(x, y)[0, 1]`     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1> Q&A 2</h1> </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Onwards!**\n",
    "\n",
    "We have now laid the theoretical groundwork. In the data set we present below and subsequent analysis, we will use the plug-in principle to deduce properties about the generative distribution. For each set of measurements we consider, we will use the empirical distribution as a plug-in replacement for the unknown generative distribution and compute relevant properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ijg5wUCTQYG"
   },
   "source": [
    "## **The Dataset**\n",
    "\n",
    "The data set in this webinar comes from a study by [Beattie, et al.](https://doi.org/10.1098/rsos.160321) in which they used the [Glasgow Facial Matching Test](https://en.wikipedia.org/wiki/Glasgow_Face_Matching_Test) (GFMT, [original paper](https://doi.org/10.3758/BRM.42.1.286)) to investigate how sleep deprivation affects a human subject's ability to match faces, as well as the confidence the subject has in those matches. Briefly, the test works by having subjects look at a pair of faces. Two such pairs are shown below.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/Hacker-Stats-in-Python-Live-Training/blob/master/assets/gfmt_faces.png?raw=True\" alt=\"GFMT faces\" width=\"400px\">\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "\n",
    "For each of 40 pairs of faces, the subject gets as much time as he or she needs and then says whether or not they are the same person. The subject then rates his or her confidence in the choice.\n",
    "\n",
    "In this study, subjects also took surveys to determine properties about their sleep. The surveys provide three different metric of sleep quality and wakefulness. \n",
    "\n",
    "- The Sleep Condition Indicator (SCI) is a measure of insomnia disorder over the past month. High scores indicate better sleep and scores of 16 and below indicate insomnia. \n",
    "- The Pittsburgh Sleep Quality Index (PSQI) quantifies how well a subject sleeps in terms of interruptions, latency, etc. A higher score indicates poorer sleep. \n",
    "- The Epworth Sleepiness Scale (ESS) assesses daytime drowsiness. Higher scores indicate greater drowsiness.\n",
    "\n",
    "We will explore how the various sleep metrics are related to each other and how sleep disorders affect subjects' ability to discern faces and their confidence in doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMYfcKeDY85K"
   },
   "source": [
    "### **Loading and inspecting the data set**\n",
    "\n",
    "Let's load in the data set provided by Beattie and coworkers. We'll load in the data set and check out the first few rows using the `head()` method of pandas DataFrames. Importantly, missing data in this data set are denoted with an asterisk, which we specify using the `na_values` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "l8t_EwRNZPLB",
    "outputId": "36a85c6f-f2ae-44e0-ac01-fc55462bc616"
   },
   "outputs": [],
   "source": [
    "# Read in the dataset\n",
    "df = pd.read_csv(\n",
    "    \"https://github.com/datacamp/Hacker-Stats-in-Python-Live-Training/blob/master/data/gfmt_sleep.csv?raw=True\",\n",
    "    na_values=\"*\",\n",
    ")\n",
    "\n",
    "# Print header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some information about what is contained in each of the columns.\n",
    "\n",
    "- `participant number`: Unique identifier for each subject\n",
    "- `age`: Age of subject in years\n",
    "- `correct hit percentage`: Percentage of correct responses among trials for which the faces match\n",
    "- `correct reject percentage`: Percentage of correct responses among trials for which the faces do not match\n",
    "- `percent correct`: Percentage of correct responses among all trials\n",
    "- `confidence when correct hit`: Average confidence when the subject gave a correct response for trials for which the faces match\n",
    "- `confidence when incorrect hit`: Average confidence when the subject gave an incorrect response for trials for which the faces match\n",
    "- `confidence when correct reject`: Average confidence when the subject gave a correct response for trials for which the faces do not match\n",
    "- `confidence when incorrect reject`: Average confidence when the subject gave an incorrect response for trials for which the faces do not match\n",
    "- `confidence when correct`: Average confidence when the subject gave a correct response for for all trials\n",
    "- `confidence when incorrect`: Average confidence when the subject gave a correct response for for all trials\n",
    "- `sci`: The subject's Sleep Condition Indicator.\n",
    "- `psqi`: The subject's Pittsburgh Sleep Quality Index.\n",
    "- `ess`: The subject's Epworth Sleepiness Scale.\n",
    "\n",
    "Going forward, it will be useful to separate the subjects into two groups, insomniacs and normal sleepers. We will therefore add an `'insomnia'` column to the DataFrame with True/False entries. Recall that a person is deemed an insomniac if their SCI is 16 or below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to the data frame for insomnia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know how many total subjects are included, so we can check on the length of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entries in data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 102 subjects, hopefully enough to make meaningful comparisons.\n",
    "\n",
    "With our data set in place, we can get moving with statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Our approach**\n",
    "\n",
    "In this live training, we will use statistical inference to address the following question of the data:\n",
    "\n",
    "1. How different is the facial matching performance of insomniacs versus normal sleepers?\n",
    "\n",
    "In our next live training, we will address other aspects of the data set.\n",
    "\n",
    "2. How different is *confidence* in facial matching for insomniacs versus normal sleepers?\n",
    "3. How are the different sleep metrics correlated?\n",
    "4. How do sleep metric *correlate* with facial matching performance?\n",
    "\n",
    "Each question requires a different sort of analysis involving calculation of confidence intervals (this session) and p-values (the next session). Along the way, we will introduce the necessary theoretical and technical concepts.\n",
    "\n",
    "Note that even though this webinar is about statistical inference, is is always important to do EDA first. Remember what [John Tukey](https://en.wikipedia.org/wiki/John_Tukey) said,\n",
    "\n",
    "> \"Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone.\"\n",
    "\n",
    "In each of the analyses, we will start with exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Performance of insomniacs versus normal sleepers**\n",
    "\n",
    "Our first investigation is into how well insomniacs and perform the face matching task versus normal sleepers. As or first step in exploratory data analysis, we will make a plot of the ECDF of the percent correct on the facial matching test for the two categories.\n",
    "\n",
    "As our first, step, we will compare the means of the two data sets. To do, we will extract the values of the `'percent correct'` column of the DataFrame for normal sleepers and for insomniacs. We will be sure to drop any missing data (NaNs). We will also convert these respective pandas Series to NumPy arrays, which enable faster computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract percent correct for normal sleepers\n",
    "\n",
    "# Extract percent correct for insomniacs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the ECDFs and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECDF for normal sleepers\n",
    "\n",
    "# Compute ECDF for insomniacs\n",
    "\n",
    "# Make plot of ECDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly fewer data points for insomniacs (25 versus 77 for normal sleepers), which will be important to consider as we do statistical inference. In eyeballing the ECDFs, it appears that those without insomnia perform a bit better; the ECDF is shifted rightward toward better scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plug-in estimates**\n",
    "\n",
    "We have already computed a plug-in estimate! The ECDF itself is a plug-in estimate for the CDF. From the ECDF, we can also directly read off plug-in estimates for any percentile. For example, the median is the 50th percentile; it is the percent correct where the ECDF is 0.5. That is, half of the measurements lie below and half above. The median for normal sleepers is 85 and that for insomniacs is 75, a 10% difference.\n",
    "\n",
    "We can also get plug-in estimates for the mean. These we can't read directly off of the ECDF, but can compute them using the `np.mean()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug in estimates for means\n",
    "\n",
    "# Print the results\n",
    "print(\"Plug-in estimates for the mean pecent correct:\")\n",
    "print(\"normal sleepers:\", pcorr_normal_mean)\n",
    "print(\"insomniacs:     \", pcorr_insom_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is about a 5% difference in the mean scores. In looking at the ECDFs, it seems like this (or the median) might be a good difference to use to compare insomniacs and normal sleepers because the ECDFs are similar at the tails (low and high percent correct), but differ in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing a confidence interval**\n",
    "\n",
    "So, we are now faced with the question: If I were to do the same experiment again, how much variation would I get in the mean percent correct? Might we again see that the insomniacs perform more poorly?\n",
    "\n",
    "To answer this questions, we can compute a **confidence interval**. A **confidence interval** can be defined as follows.\n",
    "\n",
    ">If an experiment is repeated over and over again, the estimate I compute will lie between the bounds of the 95% confidence interval for 95% of the experiments.\n",
    "\n",
    "So, all we have to do it go to Scotland, randomly select 102 people, 27 of whom are insomniacs, and perform the face matching test, record the results, and compute the mean for insomniacs and normal sleepers. Then, we have to go back to Scotland, do the whole procedure again, and do that again, and again, and again. Simple, right? \n",
    "\n",
    "Of course, we can't do that! But remember that performing an experiment is the same thing as drawing random samples out of the generative distribution. Because the generative distribution is unknown, the only way we know how to sample out of it is to literally do the experiment again, which is just not possible. However, we can use the plug-in principle to *approximate* the generative distribution with the empirical distribution. We *can* sample out of the empirical distribution using NumPy's random number generation! A sample of a new data set drawn from the empirical distribution is called a **bootstrap sample**. \n",
    "\n",
    "Imagine we have set of measurements stored in NumPy array `data`. To get a bootstrap sample, we use `np.random.choice()` to draw `len(data)` numbers out of the array `data`. We do this *with replacement*. The result is a bootstrap sample. The syntax is\n",
    "\n",
    "    bs_sample = np.random.choice(data, len(data))\n",
    "    \n",
    "The bootstrap sample is approximately a new data set drawn from the generative distribution.\n",
    "\n",
    "After drawing a bootstrap sample, we want to compute the mean in order to see how it will change from experiment to experiment. A mean (or other value of interest) computed from a bootstrap sample is referred to as a **bootstrap replicate**. We can write a function to compute a bootstrap replicate. This function takes as arguments a 1D array of data `data` and a function `func` that is to be applied to the bootstrap samples to return a bootstrap replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_replicate_1d(data, func):\n",
    "    \"\"\"Generate bootstrap replicate of 1D data.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to compute many of these replicates so we can see what range of values for the mean comprises the middle 95%, which is give a 95% confidence interval. It is therefore useful to write a function to draw many bootstrap replicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bs_reps(data, func, size=1):\n",
    "    \"\"\"Draw `size` bootstrap replicates.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Let's put these functions to use to draw bootstrap replicates of the mean for normal sleepers and for insomniacs. Because this calculation is fast, we can \"do\" the experiment over and over again many times. We'll do it 10,000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bootstrap replicates for the mean\n",
    "\n",
    "# Take a quick peak\n",
    "bs_reps_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The replicates are stored in NumPy arrays of length 10,000. The values hover around the means, but they do vary.\n",
    "\n",
    "We can compute the percentiles of the bootstrap replicates using the `np.percentile()` function. We pass in the array we want to compute percentiles for, followed by a list of the percentiles we want. For a 95% confidence interval, we can use `[2.5, 97.5]`, which will give the middle 95% of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 95% confidence intervals of the mean\n",
    "\n",
    "# Print confidence intervals\n",
    "print(\"Normal sleepers:\", conf_int_normal)\n",
    "print(\"Insomniacs:     \", conf_int_insom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 95% confidence interval of the mean for normal sleepers ranges over 5%, from 79 to 84%. That for insomniacs is twice as wide, ranging from 71 to 81%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1> Q&A 3</h1> </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing confidence intervals**\n",
    "\n",
    "The confidence intervals we printed above are useful, but the confidence intervals are perhaps better visualized graphically. The function below generates a plot of confidence intervals. Since our focus here is primarily on the concepts behind inference, you may for now take the plotting function as a black box. Briefly, we are using the more object-oriented mode of plotting with Matplotlib where we first generate a `Figure` object and an `AxesSubplot` object using `plt.subplots()`. We then use the methods of the `AxesSubplot` object to populate the plot with markers and to make further modification to the plot, such as adding axis labels. We plot the plug-in estimate (in this case, the mean) as a dot and the confidence interval as a line.\n",
    "\n",
    "The function takes as arguments a list of categories, a list of plug-in estimates, and a list of confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_ints(categories, estimates, conf_ints, palette=None):\n",
    "    \"\"\"Plot confidence intervals with estimates.\"\"\"\n",
    "    # Set a nice color palette\n",
    "    if palette is None:\n",
    "        palette = [\n",
    "            \"#1f77b4\",\n",
    "            \"#ff7f0e\",\n",
    "            \"#2ca02c\",\n",
    "            \"#d62728\",\n",
    "            \"#9467bd\",\n",
    "            \"#8c564b\",\n",
    "            \"#e377c2\",\n",
    "            \"#7f7f7f\",\n",
    "            \"#bcbd22\",\n",
    "            \"#17becf\",\n",
    "        ]\n",
    "    elif type(palette) == str:\n",
    "        palette = [palette]\n",
    "    palette = palette[: len(categories)][::-1]\n",
    "\n",
    "    # Set up axes for plot\n",
    "    fig, ax = plt.subplots(figsize=(5, len(categories) / 2))\n",
    "\n",
    "    # Plot estimates as dots and confidence intervals as lines\n",
    "    for i, (cat, est, conf_int) in enumerate(\n",
    "        zip(categories[::-1], estimates[::-1], conf_ints[::-1])\n",
    "    ):\n",
    "        color = palette[i % len(palette)]\n",
    "        ax.plot(\n",
    "            [est],\n",
    "            [cat],\n",
    "            marker=\".\",\n",
    "            linestyle=\"none\",\n",
    "            markersize=10,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "        ax.plot(conf_int, [cat] * 2, linewidth=3, color=color)\n",
    "\n",
    "    # Make sure margins look ok\n",
    "    ax.margins(y=0.25 if len(categories) < 3 else 0.125)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right! Let's use this to make a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in the length of the confidence interval is starkly apparent on the plot. Because we had fewer measurements for insomniacs, we can't be too precise in what mean values we might get.\n",
    "\n",
    "In looking at the plot of confidence intervals, it seems possible that if we did the experiment again, we might even get a scenario where insomniacs perform *better* than normal sleepers. But how likely is such a scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Confidence interval for difference of means**\n",
    "\n",
    "Remember that we are not restricted as to what confidence intervals we can compute. We can instead compute a confidence interval on the *difference* of means between normal sleepers and insomniacs. To do this, we do the following procedure, which again uses the plug-in principle to \"do\" the experiment again to get a bootstrap replicate of the difference of means.\n",
    "\n",
    "1. Generate a bootstrap sample of percent correct for normal sleepers.\n",
    "2. Generate a bootstrap sample of percent correct for insomniacs.\n",
    "3. Take the mean of each bootstrap sample, giving a bootstrap replicate for the mean of each.\n",
    "4. Subtract the mean for insomniacs from that of normal sleepers.\n",
    "\n",
    "This is actually trivial to do now because we have already computed and stored bootstrap replicates of the means! We simply have to subtract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bootstrap replicates for difference of means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the confidence interval by finding the percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confidence interval from bootstrap replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence interval just barely crosses zero, suggesting that the insomniacs will rarely perform better than normal sleepers. \n",
    "\n",
    "We can find out the *probability* of having the insomniacs perform better than the normal sleepers by counting how many times the mean percent correct for insomniacs exceeded that of normal sleepers and dividing by the total number of bootstrap replicates. To do the count, we can make an array containing `True` and `False` values for whether or not the difference of means is negative and sum the result (since `True` is worth 1 and `False` is worth 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probability of having insomniacs have better mean score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we were to do the experiment again, there is about a 3% chance we would observe the insomniacs performing at parity of better than the normal sleepers, at least based on the observations we have. If we did more actual observations, this chance could rise or fall; we cannot know without more measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summarizing the results in a report**\n",
    "\n",
    "There are many opinions about displaying the results of an analysis like this one. For me, the ECDFs are the most instructive part of our analysis, and I think they should be the central point of the discussion. In my experience, I have met resistance on presenting ECDFs because they are not in as common use as, say, bee swarm (strip) plots, histograms, box plots, or bar graphs. I get the argument that because they are less common, other people may find them difficult to interpret.\n",
    "\n",
    "With the exception of the bee swarm plots, all of these kinds of plots fail to plot all of the data. You (or someone in your organization) spent a lot of time and money to get you the data; you should display it all if you can.\n",
    "\n",
    "The bee swarm plots, while useful visualizations, are not as clear as ECDFs in showing of the data are distributed. Remember the task of statistical inference: You are trying to learn about the (unknown) generative distribution. The ECDF is an approximation of its CDF, made by plotting all of your data. You can't really get better than that.\n",
    "\n",
    "I therefore advocate for educating your organization in reading and interpreting ECDFs. They are exceptionally effective graphics, and time spent learning how to interpret them is well worth it.\n",
    "\n",
    "In addition to the ECDFs, I would also include the summary plot of the 95% confidence intervals of the mean. It helps establish what would happen if we did the experiment again; how big of a difference is there in performance between normal sleepers and insomniacs compared to changes due to variation and finite sample size?\n",
    "\n",
    "So, my summary report would look something like this...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sleep deprivation and facial matching**\n",
    "\n",
    "Twenty-seven subjects suffering from insomnia and seventy-five subjects with normal sleeping patterns were subjected to the short version of the Glasgow Facial Matching Test, comparing 40 pairs of faces each. The subjects' performance was scored based on the percent of the face matching tasks they identified correctly.\n",
    "\n",
    "Below is an empirical cumulative distribution function describing the results.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/Hacker-Stats-in-Python-Live-Training/blob/master/assets/pcorr_ecdf.svg?raw=True\" alt=\"percent correct ecdf\" width=\"400px\">\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "The distribution for insomniacs is clearly shifted leftward relative to that for normal sleepers, indicating that insomniacs have poorer performance in face matching tasks. The tails of the distributions are similar; both groups have some very poor performers and some very good performers. The key difference lies in the middle of the distribution.\n",
    "\n",
    "Below is a plot of the 95% confidence interval for the mean percent correct for normal sleepers and insomniacs.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/Hacker-Stats-in-Python-Live-Training/blob/master/assets/pcorr_conf_int.svg?raw=True\" alt=\"percent correct conf int\" width=\"400px\">\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "The uncertainty in the estimate for insomniacs is due to the smaller sample size. As an estimate, the difference in mean performance in the facial matching task is about 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><h1> Q&A 4</h1> </center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Measured data come from an unknown generative distribution, and the job of statistical inference is to learn as much as we can about that generative distribution. Hacker stats enables us to use the plug-in principle, in which the generative distribution is approximated by the empirical distribution, to obtain this information using random number generation on our computers. The results are easier to come by and to understand.\n",
    "\n",
    "In this live session, we computed ECDFs and confidence intervals for univariate data. In another live session on hacker stats, we will extend these concepts to bivariate data. We will also introduce and perform null hypothesis significance tests (NHSTs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-home question\n",
    "\n",
    "There are plenty of interesting aspects of this data set to explore. For good practice, you can use what you have learned in this live training to do compute plug-in estimates confidence intervals for the percent correct for normal sleepers separated by gender? Is there a big difference between the genders? You should also make informative graphics and write a short report like the example above to share your findings."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "python_live_session_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
